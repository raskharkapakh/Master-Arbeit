\relax 
\citation{castellini2021neural}
\citation{kujawski2019deep}
\citation{lee2021deep}
\citation{ma2019phased}
\citation{pinto2021deconvoluting}
\citation{xu2021deep}
\citation{he2018deep}
\citation{ferguson2018sound}
\citation{chakrabarty2017broadband}
\citation{perotin2018crnn}
\citation{adavanne2018direction}
\citation{takeda2016sound}
\@writefile{toc}{\contentsline {section}{\numberline {1}State-of-the-art}{1}{}\protected@file@percent }
\citation{castellini2021neural}
\citation{lee2021deep}
\citation{ma2019phased}
\citation{xu2021deep}
\citation{neekhara2019expediting}
\citation{NEURIPS2019_6804c9bc}
\citation{engel2019gansynth}
\citation{neekhara2019expediting}
\citation{NEURIPS2019_6804c9bc}
\citation{engel2019gansynth}
\citation{neekhara2019expediting}
\citation{vargas2021improved}
\citation{papayiannis2019data}
\citation{ratnarajah2021fast}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}DL-based data generation}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.1}Generation of Signal}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.2}Generation of Impulse Response}{2}{}\protected@file@percent }
\citation{bianco2020semi}
\citation{gerstoft2020parametric}
\citation{hubner2021efficient}
\citation{vera2021acoustic}
\citation{goodfellow2020generative}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.3}Generation of potential NN feature}{3}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.4}Other possible approaches to generate the data}{3}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Fundamentals}{3}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}GAN}{3}{}\protected@file@percent }
\citation{radford2015unsupervised}
\citation{goodfellow2020generative}
\citation{radford2015unsupervised}
\citation{arjovsky2017wasserstein}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}DCGAN}{4}{}\protected@file@percent }
\citation{arjovsky2017wasserstein}
\citation{arjovsky2017wasserstein}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}WGAN}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1}The Earth-Mover or Wasserstein distance}{5}{}\protected@file@percent }
\citation{DBLP:journals/corr/GulrajaniAADC17}
\citation{DBLP:journals/corr/GulrajaniAADC17}
\citation{DBLP:journals/corr/GulrajaniAADC17}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.2}Necessary changes to turn a GAN into a WGAN}{6}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}WGAN-GP}{6}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.1}The gradient penalty}{6}{}\protected@file@percent }
\citation{DBLP:journals/corr/GulrajaniAADC17}
\citation{DBLP:journals/corr/GulrajaniAADC17}
\citation{DBLP:journals/corr/GulrajaniAADC17}
\citation{arjovsky2017wasserstein}
\citation{arjovsky2017wasserstein}
\citation{gerstoft2020parametric}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}On GAN/WGAN/WGAN-GP Convergence}{7}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Our approach (\textbf  {TODO: title to change})}{8}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Data}{8}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1}Synthetic data}{8}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.2}Measurement}{8}{}\protected@file@percent }
\citation{nain2020wgangp}
\citation{nain2020wgangp}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Example of the measurement setup used, with three audio sources}}{9}{}\protected@file@percent }
\newlabel{fig:full_measurement_setup}{{1}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Generation of Eigenvalues}{9}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}WGAN-GP}{9}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Networks Architecture}{9}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Device used as audio source to create the measurement}}{10}{}\protected@file@percent }
\newlabel{fig:source}{{2}{10}}
\citation{nain2020wgangp}
\citation{arjovsky2017wasserstein}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Picture of the array of microphone used to create the real measurement}}{11}{}\protected@file@percent }
\newlabel{fig:microphone_array}{{3}{11}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.3}Performance}{11}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Architecture of the generator used in the WGAN-GP to generate eigenvalues. Total params: 283,972, Trainable params: 281,538, Non-trainable params: 2,434}}{12}{}\protected@file@percent }
\newlabel{tab:evals_generator_WGANGP_architecture}{{1}{12}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Architecture of the critic used in the WGAN-GP to generate eigenvalues. Total params: 4,303,873, Trainable params: 4,303,873, Non-trainable params: 0}}{12}{}\protected@file@percent }
\newlabel{tab:evals_critic_WGANGP_architecture}{{2}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Full structure of the used implementation of the WGAN-GP to generate eigenvalues.}}{13}{}\protected@file@percent }
\newlabel{fig:evals_wgangp_full_structure}{{4}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces The loss function of the generator while training the WGAN-GP for eigenvalues generation.}}{13}{}\protected@file@percent }
\newlabel{fig:evals_g_loss}{{5}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Top row shows eigenvalues generated by the WGAN-GP, respectively in normal values, followed by their representation in dB. The second shows the same for real eigenvalues. }}{14}{}\protected@file@percent }
\newlabel{fig:real_vs_generated_evals}{{6}{14}}
\citation{nain2020wgangp}
\citation{nain2020wgangp}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.4}Generating eigenvalues from their spectrum}{15}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Generation of Eigenvectors}{15}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1}WGAN-GP}{15}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.2}Architecture}{15}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Architecture of the generator used in the WGAN-GP to generate eigenvectors. Total params: 2,532,676, Trainable params: 2,499,522, Non-trainable params: 33,154}}{16}{}\protected@file@percent }
\newlabel{tab:evecs_generator_WGANGP_architecture}{{3}{16}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Architecture of the critic used in the WGAN-GP to generate eigenvectors. Total params: 4,317,761, Trainable params: 4,317,761, Non-trainable params: 0}}{16}{}\protected@file@percent }
\newlabel{tab:evecs_critic_WGANGP_architecture}{{4}{16}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Full structure of the used implementation of the WGAN-GP to generate eigenvectors.}}{17}{}\protected@file@percent }
\newlabel{fig:evecs_wgangp_full_structure}{{7}{17}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.3}Performance}{17}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Data Augmentation}{17}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.1}Comparison between augmented data and real data.}{17}{}\protected@file@percent }
\bibdata{../mybib}
\bibcite{adavanne2018direction}{{1}{2018}{{Adavanne et~al.}}{{Adavanne, Politis, and Virtanen}}}
\bibcite{arjovsky2017wasserstein}{{2}{2017}{{Arjovsky et~al.}}{{Arjovsky, Chintala, and Bottou}}}
\bibcite{bianco2020semi}{{3}{2020}{{Bianco et~al.}}{{Bianco, Gannot, and Gerstoft}}}
\bibcite{castellini2021neural}{{4}{2021}{{Castellini et~al.}}{{Castellini, Giulietti, Falcionelli, Dragoni, and Chiariotti}}}
\bibcite{chakrabarty2017broadband}{{5}{2017}{{Chakrabarty and Habets}}{{}}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Results of beamforming from a real CSM}}{18}{}\protected@file@percent }
\newlabel{fig:beamforming_real_csm}{{8}{18}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Results of beamforming from a CSM issued from augmented data}}{18}{}\protected@file@percent }
\newlabel{fig:beamforming_augmented_csm}{{9}{18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Generation of Cross-Correlation Matrix}{18}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Future work}{18}{}\protected@file@percent }
\bibcite{engel2019gansynth}{{6}{2019}{{Engel et~al.}}{{Engel, Agrawal, Chen, Gulrajani, Donahue, and Roberts}}}
\bibcite{ferguson2018sound}{{7}{2018}{{Ferguson et~al.}}{{Ferguson, Williams, and Jin}}}
\bibcite{gerstoft2020parametric}{{8}{2020}{{Gerstoft et~al.}}{{Gerstoft, Groll, and Mecklenbr{\"a}uker}}}
\bibcite{goodfellow2020generative}{{9}{2020}{{Goodfellow et~al.}}{{Goodfellow, Pouget-Abadie, Mirza, Xu, Warde-Farley, Ozair, Courville, and Bengio}}}
\bibcite{DBLP:journals/corr/GulrajaniAADC17}{{10}{2017}{{Gulrajani et~al.}}{{Gulrajani, Ahmed, Arjovsky, Dumoulin, and C.Courville}}}
\bibcite{he2018deep}{{11}{2018}{{He et~al.}}{{He, Motlicek, and Odobez}}}
\bibcite{hubner2021efficient}{{12}{2021}{{H{\"u}bner et~al.}}{{H{\"u}bner, Mack, and Habets}}}
\bibcite{kujawski2019deep}{{13}{2019}{{Kujawski et~al.}}{{Kujawski, Herold, and Sarradj}}}
\bibcite{NEURIPS2019_6804c9bc}{{14}{2019}{{Kumar et~al.}}{{Kumar, Kumar, de~Boissiere, Gestin, Teoh, Sotelo, de~Br\'{e}bisson, Bengio, and Courville}}}
\bibcite{lee2021deep}{{15}{2021}{{Lee et~al.}}{{Lee, Chang, and Lee}}}
\bibcite{ma2019phased}{{16}{2019}{{Ma and Liu}}{{}}}
\bibcite{nain2020wgangp}{{17}{2020}{{Nain}}{{}}}
\bibcite{neekhara2019expediting}{{18}{2019}{{Neekhara et~al.}}{{Neekhara, Donahue, Puckette, Dubnov, and McAuley}}}
\bibcite{papayiannis2019data}{{19}{2019}{{Papayiannis et~al.}}{{Papayiannis, Evers, and Naylor}}}
\bibcite{perotin2018crnn}{{20}{2018}{{Perotin et~al.}}{{Perotin, Serizel, Vincent, and Gu{\'e}rin}}}
\bibcite{pinto2021deconvoluting}{{21}{2021}{{Pinto et~al.}}{{Pinto, Bauerheim, and Parisot-Dupuis}}}
\bibcite{radford2015unsupervised}{{22}{2015}{{Radford et~al.}}{{Radford, Metz, and Chintala}}}
\bibcite{ratnarajah2021fast}{{23}{2021}{{Ratnarajah et~al.}}{{Ratnarajah, Zhang, Yu, Tang, Manocha, and Yu}}}
\bibcite{takeda2016sound}{{24}{2016}{{Takeda and Komatani}}{{}}}
\bibcite{vargas2021improved}{{25}{2021}{{Vargas et~al.}}{{Vargas, Hopgood, Brown, and Subr}}}
\bibcite{vera2021acoustic}{{26}{2021}{{Vera-Diaz et~al.}}{{Vera-Diaz, Pizarro, and Macias-Guarasa}}}
\bibcite{xu2021deep}{{27}{2021}{{Xu et~al.}}{{Xu, Arcondoulis, and Liu}}}
\bibstyle{plainnat}
\gdef \@abspage@last{20}
