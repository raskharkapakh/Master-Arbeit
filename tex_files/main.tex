\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{cite}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{comment}

%\geometry{
%    a4paper,
%    total={170mm, 257mm},
%    left=20mm,
%    top=10mm,
%}

\title{Master Thesis}
\author{Gaspard Ulysse Fragni√®re}
\date{August 2022}

\begin{document}

\maketitle

\section{How to \LaTeX}

How to make a reference to a paper:\cite{grumiaux2022survey}


\section{Draft}

\subsection{Grumiaux}


\cite{grumiaux2022survey} is a survey of several methods for sound source localization (SSL). Tradiontionally, this problem has been tackled using Signal Processing based methods. But in the recent years, methods based on deep learning have been developped and showed better results than traditional approaches. Those methods have been compiled in this paper. The survey is organized in the different following sections:

\begin{itemize}
    \item \textbf{Section I}: Introduction
    \item \textbf{Section II}: Acoustic Environment and Sound Source Configuration
    \item \textbf{Section III}: Conventional SSL methods
    \item \textbf{Section IV}: Neural Network Architectures for SSL
    \item \textbf{Section V}: Input Features
    \item \textbf{Section VI}: Outputs strategies
    \item \textbf{Section VII}: Data
    \begin{enumerate}
        \item Synthetic Data
        \item Real data
        \item Data augmentation techniques
    \end{enumerate}    
        
    \item \textbf{Section VIII}: Learning Strategies
    \item \textbf{Section IX}: Conclusions and Perspectives
    
\end{itemize}

We are interested in the section about Synthetic Data and Data augmentation. Indeed those sections can be used as a statring point for building the state of the art.Its goal is to answer the following questions:


\begin{itemize}
    \item Are there \textbf{existing methods} to generate virtually:
    \begin{itemize}
        \item measured time data (single channel/multi-channel)?
        \item measured source spectra (single channel/multi-channel)?
        \item measured cross-spectral matrices in stationary environments (multi-channel only)?
        
    \end{itemize}
    \item What \textbf{measurement scenarios} are used in the literature (time-stationary/non-stationary sources, number of microphones, temporal dimensions... )?
    \item What are the \textbf{existing setups} in multi-channel data generation with neural networks (conditioning variables, network architectures (convolutional, recurrent, Transformer,\dots), generative algorithms (GAN/VAE), \dots)
\end{itemize}

In \cite{grumiaux2022survey}, a classical method about data generation is introduced. The idea is the following: Simulate the Room Impulse Response (RIR) in order to simulate realistically room acoustics (e.g. reverberation). Then convolve dry audio signals with the RIR simulated. This can provide suited training data, since RIR for rooms of different size, different source prosition as well as different dry signals can be used for the training.

In  \cite{deleforge2015co}, the datasets is created in the following way: a speaker with a visual marker is placed in front of camera and binaural microphone setup (dummy head). "The loud-speaker that emits fixed-length full-spectrum sounds is moved in front of the dummy-head/camera device and for each loud-speaker location, both the emitted sound and the image location of the visual marker are recorded. $\rightarrow$ not so useful

In \cite{vargas2021improved}, a GAN is used to simulate data. The GAN used in \cite{vargas2021improved} is an implementation of \cite{neekhara2019expediting}

\cite{hubner2021efficient} proposed a low-complexity model-based training data generation method that includes a deterministic model for the direct path and a statistical model for late reverberation. It has been demonstrated that the SSL neural network, trained using the data generated by this method, achieves comparable localization performance as the same architecture trained on a dataset generated by the usual ISM.

An investigation of several simulation methods was done by \cite{gelderblom2021synthetic}, with extensions of ISM, namely, ISM with directional sources, and ISM with a diffuse field due to scattering. \cite{gelderblom2021synthetic} compared the simulation algorithms via the training of an MLP (in both regression and classification modes) and showed that ISM with scattering effects and directional sources leads to the best SSL performance.

\cite{neekhara2019expediting} : We propose a learning-based method which uses Generative Adversarial Networks [12] to learn a stochastic mapping from perceptually-informed spectrograms into simple magnitude spectrograms.

Paper read and not useful:

\begin{itemize}
    \item Deleforge 2013 \cite{deleforge2013variational}
    \item 
\end{itemize}

\subsection{Bianco}

\begin{itemize} 
    \item \textbf{in:} Relative Transfer Function (RTF)
    \item \textbf{out:} Relative Transfer Function (RTF)
    \item \textbf{measurement scenario:} Binaural microphone.
    \item \textbf{setup:} VAE (Semi supervised learning)
\end{itemize}


In \cite{bianco2020semi} a VAE is used to perform SSL. The idea is the following: based on VAEs to encode the phase of the relative transfer function (RTF) between two microphones to a latent parametric distribution. The resulting model estimates DOA and generates RTF phase.

VAEs learn from unlabeled data explicit latent codes for generating samples, and are
inspiring examples of representation learning. 

There is a link between DOA and RTF. Indeed, the RTF phase is encoded as a function of source azimuth (direction of arrival, DOA). Similarly as in \cite{gerstoft2020parametric}, the goal of the NN (GAN or VAE) is to learn the distribution of a a quantity that is a function of the DOA. 

The experiments show, only \textbf{two labeled samples per DOA} permit the VAE-SSL to obtain better performance than SRP-PHAT (State of the art).

\subsubsection{Bianco: the maths:}

The goal in  \cite{bianco2020semi} is to create a VAE to generate an acoustics feature. The acoustics feature of interests here is the Relative Transfer Function (RTF). Model: we consider the following model:

\begin{equation}
    d_i = s \ast a_i + u_i 
\end{equation}

with \begin{itemize}
    \item $i \in \{1,2\}$: the microphone index
    \item $d_i$: time domain acoustic recording at microphone $i$
    \item $s$ the acoustic source
    \item $a_i$: the impulse response (IR) at microphone $i$  
    \item $u_i$: the noise at microphone $i$ 
\end{itemize}

Then we can define the RTF $H(k)$ as

\begin{equation}
    H(k) = \frac{A_1(k)}{A_2(k)}
\end{equation}

with $A_i(k)$ the Fourier transform of $a_i$ and $k$ the frequency.

Then $H(k)$ can be estimated by (with $d_1$ as reference):

\begin{equation}
        \hat{H}(k) = \frac{S_{d1d2}}{S_{d1d1}}
\end{equation}

with 

\begin{itemize}
    \item $S_{d1d1} = D_1(k)^{\ast} D_1(k)$: the Power Spectral Density (PSD)
    \item $S_{d2d1} = D_2(k)^{\ast} D_1(k)$: the cross-PSD for a single frame.
\end{itemize}

The estimator is biased since we ignore the PSD of the noise. For each FFT frame a vector is obtained of RTF is obtained $\hat{\mathbf{h}} = [\hat{H}(1), \dots, \hat{H}(K)]^T \in \mathbb{C}^K$ for $K$ frequencies bin. We use RTFs estimated using a single frame as input to the VAE-SSL.

the $n$th input sample and the supervised CNN is a sequence of RTFs frames :

\begin{equation}
        \mathbf{x}_n = \text{vec}(\text{phase}(\hat{\mathbf{H}}_n)) \in \mathbb{R}^{KP}
\end{equation}

with 

\begin{itemize}
    \item $\hat{\mathbf{H}}_n = [\hat{\mathbf{h}}_n \dots \hat{\mathbf{h}}_{n+P-1}] \in \mathbb{C}^{K \times P}$
    \item $K = N_{\text{FFT}}/2$
    \item $P$: the number of RTF frame in the sequence. 
\end{itemize}

\subsection{Neekhara}

\begin{itemize}
    \item \textbf{in:} (perceptually informed) spectogram (or text on a more basic level)
    \item \textbf{out:} natural sounding audio waveform
    \item \textbf{measurement scenario:} -
    \item \textbf{setup:} GAN (for amplitude estimation)
\end{itemize}

\cite{neekhara2019expediting} is concerned with finding a solution for Text to speech (TTS) problem. The claim is that using a GAN approach, they have been able to outperform by far naive approaches (user review) and being 100x faster than other DL approaches. More specifically this paper was concerned with creating a mapping from language to \textbf{perceptually informed spectogram}. Indeed the difficluty of the problem lays in the fact that perceptually informed spectograms are not invertible. Indeed a spectogram is a compact representation of a signal where much of the information contained in a audio waveform has been lost. More specifically the problem at hand is phase estimation and magnitude estimation. Therefore a predictive model is required to fill the missing information and create natural sounding sound.

\subsection{NEURIPS}

\begin{itemize}
    \item \textbf{in:} mel-spectogram 
    \item \textbf{out:} audio waveform
    \item \textbf{measurement scenario:} -
    \item \textbf{setup:} non-autoregressive feed-forward convolutional architecture to
    perform audio waveform generation in a GAN setup
\end{itemize}

\cite{NEURIPS2019_6804c9bc} also introduces a method for the TTS problem. Moreover in the introduction of the paper, there is a comparison of different method for text to speech (i.e. audio wave generation):

\begin{itemize}
    \item \textbf{Pure signal processing approaches:} "The main issue with these pure signal processing methods is that the mapping from intermediate features to audio usually \textbf{introduces noticeable artifacts}"
    \item \textbf{Autoregressive NN based models:} An autoregressive model is a model that relies on past values to predict current ones. In this sense, an autoregressive model must be sequential. "These methods have produced state-of-the-art results in text-to-speech synthesis and other audio generation tasks. Unfortunately, inference with these models is inherently slow and inefficient because audio samples must be generated sequentially. Thus auto-regressive models are \textbf{usually not suited for real-time applications}."

    \item \textbf{Non autoregressive models:} "While inference is fast on the GPU, the large size of the model makes it \textbf{ impractical for applications with a constrained memory budget.}"
    \item \textbf{GAN for audio:} However their results show that adversarial loss alone is not sufficient for high quality waveform generation; it requires a  KL-divergence based distillation objective as a critical component. To this date, making them work well in this domain has been challenging
\end{itemize}

In \cite{NEURIPS2019_6804c9bc}, a GAN (MelGAN) is introduced. The goal of this GAN is to perform audio waveform generation.

\subsection{EngelGanSynth:}

\begin{itemize}
    \item \textbf{in:} instruments note from datasets NSynth (Single channel )
    \item \textbf{out:} instruments audio waveform.
    \item \textbf{measurement scenario:} - 
    \item \textbf{setup:} GAN
\end{itemize}

\cite{engel2019gansynth} is concerned with demonstrating that GANs can in fact generate high-fidelity and locally-coherent naudio by modeling log magnitudes and instantaneous frequencies with sufficient frequency resolution in the spectral domain.

Using a GAN approach and the NSythn datasets (Dataset of standardized notes from instrument) to generate notes. 

TODO: According to Adam, this paper is to generate spectogram and not audio wave form. $\rightarrow$ check again

\subsection{gerstoft}

\begin{itemize}
    \item \textbf{in:} True Sample Covariance Matrix with DOA
    \item \textbf{out:} Generator, i.e. probability distribution function
    \item \textbf{measurement scenario:} -
    \item \textbf{setup:} (Wasserstein) GAN
\end{itemize}

\cite{gerstoft2020parametric} is concerned with training a GAN for generating audio based features. 

In many array processing techniques (i.e. beamfroming) first compute the Sample Covariance Matrix (SCM). The idea of the (Wasserstein) GAN here is to generate many SCM. More specifically, the goal of the GAN is to learn the \textbf{joint} probability distribution function of the \textbf{observable data} (array data) and the \textbf{target variable} (Wasserstein distance). The wasserstein distance is a metric to measure distance between probability distribution on a given metric space.

The idea is first to consider a model that describe relationship between \textbf{array data} and \textbf{DOA}. Using this model, another relationship betwen \textbf{CSM} and \textbf{array data} can be defined. Hence we have a mathematical relationship between \textbf{DOA} and \textbf{CSM}.

Idea: maybe it would be possible to generate for the pdf os SCM for several DOA. Moreover, a link between SCM and cross spectral matrix should be investigated.

\subsection{Vera-Diaz}

\begin{itemize}
    \item \textbf{in:} GCC
    \item \textbf{out:} (x,y,z)
    \item \textbf{measurement scenario:} pair of microphone
    \item \textbf{setup:} (Deep) CNN
\end{itemize}

In ASL prblem, the source‚Äôs position can be estimated with at least three Time Difference of Arrival (TDoA) measurements with hyperbolic trilateration methods. Signals captured in everyday scenarios are contaminated with noise and multipath effects.
Directly measuring TDoA in those cases is a diÔ¨Écult task that produces inaccurate localization results.\cite{vera2021acoustic} is concerned with finding a way to denoise those signals. Other methods uses GCC instead of TDoA. SUch methods are more robust to noise and multipath effects, but not fully immune to it. 

The contribution of this paper is a DNN named DeepGCC. This DNN takes as input a GCC(Generalized Cross Correlation) Matrix and estimates a Gaussian function. Then the SSL problem is solved by replacing the GCC typically used by DeepGCC(GCC) and then using a classical beamforming approach. 

$\rightarrow$ this is actually the inverse of what we want. Hence if we could reverse this nectwork, maybe we could do something of the following: 

\begin{equation}
    (x,y,z) \rightarrow GCC \rightarrow CSM
\end{equation}

Where the last step is done thanks to the relation between $GCC$ and $CSM$ via Fourier transform. 

\subsection{H√ºbner}

\begin{itemize}
    \item \textbf{in:} -
    \item \textbf{out:} data for phase based DOA approximation. (RTF)
    \item \textbf{measurement scenario:} -
    \item \textbf{setup:} CNN
\end{itemize}

This paper is concerned with dealing with a common issue when creating DL algorithm to solve the SSL problem, namely the complexity to gather data for training/validating. Indeed, the two current way to obtain data is by either generating them using simulation or by recording them in real life. Both methods require significant amount of resources (resp. time or storage)

\textbf{TODO}: Use those quotes to structure a bit the the state-of-the-art

quotes from H√ºbner:
2
"DOA estimation methods can be categorized into classical model-based methods and data-driven methods, which are prevalently implemented using deep neural networks (DNN)."

"Most of the DL methods include a feature extraction step rather than using the raw microphone signals. Popular features include:

\begin{itemize}
    \item (i) the eigendecomposition of the spatial covariance matrix [14] (similar to MUSIC)
    \item (ii) generalized cross-correlation (GCC) based features [15‚Äì18]
    \item (iii) modal coherence [19]
    \item (iv) the Ambisonics intensity vectors [20]
    \item (v) phase and magnitude spectra [21] and
    \item (vi) phase spectra [11, 12]. Many of the features are phase-based as motivated by physical models and classical DOA estimators [9].
\end{itemize}
"

"One way to generate training data for DL-based DOA estimation is by recording sound emitted from a source (e.g., loudspeaker, human) in real acoustic environments [16, 17]. This approach is time-consuming and for high-quality datasets a precise ground truth position is essential, which requires expensive measurement equipment."

"Another popular method is the convolution of signals (e.g., speech) with room impulse responses (RIRs) that have either been recorded [14, 18] or simulated based on the source-image method [11, 12, 20, 21, 23]. The main drawbacks of these data generation methods are excessive time and storage consumption."


Conclusion: 
"We proposed a low complexity model-based training data
generation method for phase-based DOA estimation. The
proposed method models the microphone phases directly
in the frequency domain to avoid computationally costly
operations as present in state-of-the-art methods. The low
computational complexity of the proposed method allows for
online training data generation, which allows faster proto-
typing, and paves the way for \textbf{applications with a high data
demand} such as moving sound sources simulation or large
microphone arrays. An evaluation using measured RTFs
yielded \textbf{comparable results} for phase-based DOA estimation
when using \textbf{the proposed method and the computationally
expensive source-image method for training data generation.}"


\subsection{46107}

\cite{46107} is concerned with room simulation -> precisely not what we want to do. 

\subsection{measurement scenarios}

The different measurement scenarios

\subsubsection{time-stationary sources}

\subsubsection{time non-stationary sources}

\subsubsection{number of microphones}

\subsubsection{temporal dimensions} 





\subsection{Difference between VAE and GAN}

In the context of image generation:

"GANs generally produce better photo-realistic images but can be difficult to work with. Conversely, VAEs are easier to train but don‚Äôt usually give the best results.

I recommend picking VAEs if you don‚Äôt have a lot of time to experiment with GANs and photorealism isn‚Äôt paramount.

There are exceptions such as Google‚Äôs VQ-VAE 2 which can compete with GANs for image quality and realism. There is also VAE-GAN and VQ-VAE-GAN.

As a note, GANs and VAEs are not specifically for images and can be used for other data types/structures." (this is from a comment on stackstats -> not really usable as a source).

TODO: $\rightarrow$ read \cite{10.1007/978-3-030-38961-1_8}


% =======================================================================================

\section{Goal}

The goal of the project is to create a GAN to generate either:
\begin{itemize}
    \item cross-spectral matrix
    \item the complex-valued sound pressure vector at the different microphone
\end{itemize}
with the corresponding labels (DoA).

In some way the goal of this project is to create a GAN to realistically add noise to either a cross spectral matrix or complex-valued signal sound pressure vector of an array of microphone. This noise pdf should be estimated instead of being modelled. Indeed a GAN approach is necessary to reduce the computation time and the storage required. Indeed we want to be able to generate randomly and in real time labeled data.  

\section{How to train a GAN:}

\begin{enumerate}
    \item feed sample to the discriminator to make it learn what a real sample is.
    \item When the discriminator can distinguish recognize real sample, we need to feed it fake sample and make sure it can recognize them as fake
    \item When the discriminator is good enough at his job, we can start training the generator. The generator takes as input a random input vector and create a sample
    \item The knowledge whether the sample was fake or not is revealed to both networks. Based upon this, the generator and discriminator need to adapt their behaviour:
    \begin{itemize}
        \item there is always a winner and a loser
        \item if the discriminator successfully detect the image as fake, it remains unchanged an5d the generator need to change its behaviour 
        \item On the other hand, if the discriminator fail to detect the image as fake, it has to adapt its behaviour and the generator stays unchanged. 
    \end{itemize}
    \item We repeat this process until the generator is so good that the discriminator can no longer detect the fakes.
\end{enumerate}

\section{Important words and relationships}

\begin{itemize}
    \item \textbf{SSL}: Sound source localization
    \item alternatively \textbf{SSL}: Semi-Supervised Learning
    \item \textbf{ASL}: Acoustics Source Localization    
    \item \textbf{Source Spectra}
    \item \textbf{GCC}: Generalized Cross Correlation
    \item \textbf{CSM}: Cross Spectral Matrix (note that there is a connection between the CSM and the GCC features via the Fourier transform)
    \item \textbf{SCM}: Sample Covariance Matrix
    \item \textbf{CB map}: Conventional Beamforming map (also known as Acoustic Power Map???)
    \item \textbf{conditioning variable}: splitting the data up into bins based on the values of these features, and then training a model for each bin. Then examining the differences between the models. Usually this is done to learn something about the benefits of using the different features, and about the relationships between features and outputs.
    \item \textbf{(Time) Stationary process}: process with mean, variance and autocorrelation structure not changing over time (constant overtime)
    \item \textbf{Autoregressive}: An autoregressive model specifies that the output variable depends linearly on its own previous values and on a stochastic term 
    \item Data Covariance Matrix $\leftrightarrow$ Cross Spectral Matrix $\leftrightarrow$ Cross Power Spectrum.
    \item relationship between pressure vector in the microphone array $\mathbf{p}$ and the Cross-Spectral Matrix $\mathbf{C}$
    \item SED: Sound Event Detection
    
    \begin{equation}
        \mathbf{C} = \mathbf{p} \mathbf{p}^H 
    \end{equation}
\end{itemize}

\section{Papers found summary:}

%=-%=-%=-%=-%=-%=-%=-%=-%=-%=-%=-%=-%=-%=-%=-%=-%=-%=-%=-%=-%=-%=-%=-%=-%=-%=-%=-%=-%=-%=
\subsection{Bianco}

In \cite{bianco2020semi}:

\begin{itemize} 
    \item \textbf{in:} Relative Transfer Function (RTF)
    \item \textbf{out:} Relative Transfer Function (RTF)
    \item \textbf{measurement scenario:} Binaural microphone.
    \item \textbf{setup:} VAE (Semi supervised learning)
\end{itemize}

\subsection{Neekhara}

In \cite{neekhara2019expediting}:

\begin{itemize}
    \item \textbf{in:} (perceptually informed) spectogram (or text on a more basic level)
    \item \textbf{out:} natural sounding audio waveform
    \item \textbf{measurement scenario:} -
    \item \textbf{setup:} GAN (for amplitude estimation)
\end{itemize}

\subsection{NEURIPS}

In \cite{NEURIPS2019_6804c9bc}:

\begin{itemize}
    \item \textbf{in:} mel-spectogram 
    \item \textbf{out:} audio waveform
    \item \textbf{measurement scenario:} -
    \item \textbf{setup:} non-autoregressive feed-forward convolutional architecture to
    perform audio waveform generation in a GAN setup
\end{itemize}

\subsection{Engel}

in \cite{engel2019gansynth}:

\begin{itemize}
    \item \textbf{in:} instruments note from datasets NSynth (Single channel)
    \item \textbf{out:} instruments audio waveform.
    \item \textbf{measurement scenario:} - 
    \item \textbf{setup:} GAN
\end{itemize}

\subsection{Gerstoft}

In \cite{gerstoft2020parametric}:

\begin{itemize}
    \item \textbf{in:} True Sample Covariance Matrix with DOA
    \item \textbf{out:} Generator, i.e. probability distribution function for Sample Covariance Matrix.
    \item \textbf{measurement scenario:} -
    \item \textbf{setup:} (Wasserstein) GAN
\end{itemize}

\subsection{Vera-Diaz}

In \cite{vera2021acoustic}:

\begin{itemize}
    \item \textbf{in:} GCC
    \item \textbf{out:} (x,y,z)
    \item \textbf{measurement scenario:} pair of microphone
    \item \textbf{setup:} (Deep) CNN
\end{itemize}

%=-%=-%=-%=-%=-%=-%=-%=-%=-%=-%=-%=-%=-%=-%=-%=-%=-%=-%=-%=-%=-%=-%=-%=-%=-%=-%=-%=-%=-%=


\section{State-of-the-art}

The problem of Acoustical Source Localization (ASL) is an important problem. It was many applications suach as smart assistant (e.g. Google Home, Alexa, ...), industrial applications, \textbf{TODO: add more?}. Traditionnaly this problem is tackled with methods based on the physics of sound propagation (e.g. TDoA, beamforming) or with statistcal inference (e.g.Sparse Bayesian Learning). 

The recent success of Deep Learning (DL) based method in other field of research (e.g. Computer Vision) led to believe that Deep Neural Networks (DNN) based approaches could provide state-of-the-art result in solving the ASL problem. \cite{castellini2021neural}, \cite{kujawski2019deep}, \cite{lee2021deep}, \cite{ma2019phased}, \cite{pinto2021deconvoluting} and \cite{xu2021deep} propose state-of-the-arts DL-based methods for Source Characterization. It is important to note that in \cite{castellini2021neural}, \cite{lee2021deep}, \cite{ma2019phased}, \cite{xu2021deep}, the Cross Power Spectra (CPS) is used as features for source characterization. The CPS is a direct representation of the signals received in the array of microphone.

A common issue faced while implementing deep learning based methods is that significant quantities of well structures data are required. In the litterature, two main ways of obtaining data have been observed:

\begin{itemize}
    \item Record sound emitted with a loudspeaker or human voice in a real acoustic enviromnent. The issue with such methods is that it can be very tedious te record in different environment. Moreover, recording sufficiently data is very time consuming. Finally, to build a high quality data sets, expensive equipment is required to have an accurate groundtruth. In the literature, \cite{he2018deep} and \cite{ferguson2018sound} have such an approach.
    \item Simulate a Room Impulse Response (RIR) in order to recreate realistic room acoustics (e.g. reverberation). Then convolve dry audio signals with the RIR simulated. This can provide suited training data, since RIR for rooms of different size, different source prosition as well as different dry signals can be used for the training. The issue with such a method is the important amount of time and storage required. E.g. \cite{chakrabarty2017broadband}, \cite{perotin2018crnn} and \cite{adavanne2018direction} created their datasets in this way.
\end{itemize}


Therefore we would like to find another to generate datas, using Deep Learning approach. It is important to note that DL-based approach do not necessarily use raw data (direct recording of microphone input) but instead features extracted from the raw data. This is crucial because it means that recording, simulating or generating raw microphone data is no longer necessary, if good quality features could be generated directly.

\cite{neekhara2019expediting}, \cite{NEURIPS2019_6804c9bc}, \cite{engel2019gansynth} use Generative Adversial Network (GAN) to generate realistic audio waveform. \cite{neekhara2019expediting} and \cite{NEURIPS2019_6804c9bc} specifically focus on the generation of audio waveform conditioned on a spectogram (cGAN). On the other hand, \cite{engel2019gansynth} design a GAN to generate realistic audio waveform of single music notes played by an instrument. It is important to note that the GAN designed by \cite{neekhara2019expediting} is the one implemented in \cite{vargas2021improved}. 

\cite{bianco2020semi} proposes an approach to generate another acoustic feature: the phase of the relative transfer function (RTF) between two microphones. In this paper a Variational Auto Encoder (VAE) is designed to simultaneously generate phases of RTF and classifying them by their Direction of Arrival (DoA).

\cite{gerstoft2020parametric} use a GAN to generate Sample Cross Spectra Matrices (named as Sample Covariance Matrices) for a given DoA. In their approach, the GAN is trained with data only coming from one DoA, making it unable to generate sample for different DoA.

In \cite{hubner2021efficient} introduce a low complexity model-based method for generating samples of microphones phases. This method proposed is not based on DL. Indeed, it is based on a statistical noise model, a deterministic direct-path model for the point source, and a statistical model. The claim of this paper is that the low complexity of the proposed  model makes it suited for online training data generation. 

\cite{vera2021acoustic} introduce a CNN for denoising (i.e. removing the effects of reverberation and multipath effects) of of the Generalization Cross Correlation (GCC) matrix of an array of microphone. This is interesting, since such an network could maybe be inverted to noise GCC matrix and hence make them realistic. 

\cite{papayiannis2019data} introduce a GAN approach to generate artificial Acoustical Impulse Response (AIR). An AIR is high-dimensional, consisting of thousands of coefficients. AIRs are used typically in the problem of classification of acoustic environment.






% Bibliography
%\addbibresource{mybib.bib}
\bibliography{mybib}
\bibliographystyle{plain} 


\end{document}
