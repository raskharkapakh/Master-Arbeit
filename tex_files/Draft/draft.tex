\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{cite}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{comment}
%\usepackage[round]{natbib}

%\geometry{
%    a4paper,
%    total={170mm, 257mm},
%    left=20mm,
%    top=10mm,
%}

\title{Master Thesis}
\author{Gaspard Ulysse Fragnière}
\date{August 2022}

\begin{document}

\maketitle

\section{How to \LaTeX}

How to make a reference to a paper:\cite{grumiaux2022survey}


\section{Draft}

\subsection{Grumiaux}


\cite{grumiaux2022survey} is a survey of several methods for sound source localization (SSL). Tradiontionally, this problem has been tackled using Signal Processing based methods. But in the recent years, methods based on deep learning have been developped and showed better results than traditional approaches. Those methods have been compiled in this paper. The survey is organized in the different following sections:

\begin{itemize}
    \item \textbf{Section I}: Introduction
    \item \textbf{Section II}: Acoustic Environment and Sound Source Configuration
    \item \textbf{Section III}: Conventional SSL methods
    \item \textbf{Section IV}: Neural Network Architectures for SSL
    \item \textbf{Section V}: Input Features
    \item \textbf{Section VI}: Outputs strategies
    \item \textbf{Section VII}: Data
    \begin{enumerate}
        \item Synthetic Data
        \item Real data
        \item Data augmentation techniques
    \end{enumerate}    
        
    \item \textbf{Section VIII}: Learning Strategies
    \item \textbf{Section IX}: Conclusions and Perspectives
    
\end{itemize}

We are interested in the section about Synthetic Data and Data augmentation. Indeed those sections can be used as a statring point for building the state of the art.Its goal is to answer the following questions:


\begin{itemize}
    \item Are there \textbf{existing methods} to generate virtually:
    \begin{itemize}
        \item measured time data (single channel/multi-channel)?
        \item measured source spectra (single channel/multi-channel)?
        \item measured cross-spectral matrices in stationary environments (multi-channel only)?
        
    \end{itemize}
    \item What \textbf{measurement scenarios} are used in the literature (time-stationary/non-stationary sources, number of microphones, temporal dimensions... )?
    \item What are the \textbf{existing setups} in multi-channel data generation with neural networks (conditioning variables, network architectures (convolutional, recurrent, Transformer,\dots), generative algorithms (GAN/VAE), \dots)
\end{itemize}

In \cite{grumiaux2022survey}, a classical method about data generation is introduced. The idea is the following: Simulate the Room Impulse Response (RIR) in order to simulate realistically room acoustics (e.g. reverberation). Then convolve dry audio signals with the RIR simulated. This can provide suited training data, since RIR for rooms of different size, different source prosition as well as different dry signals can be used for the training.

In  \cite{deleforge2015co}, the datasets is created in the following way: a speaker with a visual marker is placed in front of camera and binaural microphone setup (dummy head). "The loud-speaker that emits fixed-length full-spectrum sounds is moved in front of the dummy-head/camera device and for each loud-speaker location, both the emitted sound and the image location of the visual marker are recorded. $\rightarrow$ not so useful

In \cite{vargas2021improved}, a GAN is used to simulate data. The GAN used in \cite{vargas2021improved} is an implementation of \cite{neekhara2019expediting}

\cite{hubner2021efficient} proposed a low-complexity model-based training data generation method that includes a deterministic model for the direct path and a statistical model for late reverberation. It has been demonstrated that the SSL neural network, trained using the data generated by this method, achieves comparable localization performance as the same architecture trained on a dataset generated by the usual ISM.

An investigation of several simulation methods was done by \cite{gelderblom2021synthetic}, with extensions of ISM, namely, ISM with directional sources, and ISM with a diffuse field due to scattering. \cite{gelderblom2021synthetic} compared the simulation algorithms via the training of an MLP (in both regression and classification modes) and showed that ISM with scattering effects and directional sources leads to the best SSL performance.

\cite{neekhara2019expediting} : We propose a learning-based method which uses Generative Adversarial Networks [12] to learn a stochastic mapping from perceptually-informed spectrograms into simple magnitude spectrograms.

Paper read and not useful:

\begin{itemize}
    \item Deleforge 2013 \cite{deleforge2013variational}
    \item 
\end{itemize}

\subsection{Bianco}

\begin{itemize} 
    \item \textbf{in:} Relative Transfer Function (RTF)
    \item \textbf{out:} Relative Transfer Function (RTF)
    \item \textbf{measurement scenario:} Binaural microphone.
    \item \textbf{setup:} VAE (Semi supervised learning)
\end{itemize}


In \cite{bianco2020semi} a VAE is used to perform SSL. The idea is the following: based on VAEs to encode the phase of the relative transfer function (RTF) between two microphones to a latent parametric distribution. The resulting model estimates DOA and generates RTF phase.

VAEs learn from unlabeled data explicit latent codes for generating samples, and are
inspiring examples of representation learning. 

There is a link between DOA and RTF. Indeed, the RTF phase is encoded as a function of source azimuth (direction of arrival, DOA). Similarly as in \cite{gerstoft2020parametric}, the goal of the NN (GAN or VAE) is to learn the distribution of a a quantity that is a function of the DOA. 

The experiments show, only \textbf{two labeled samples per DOA} permit the VAE-SSL to obtain better performance than SRP-PHAT (State of the art).

\subsubsection{Bianco: the maths:}

The goal in  \cite{bianco2020semi} is to create a VAE to generate an acoustics feature. The acoustics feature of interests here is the Relative Transfer Function (RTF). Model: we consider the following model:

\begin{equation}
    d_i = s \ast a_i + u_i 
\end{equation}

with \begin{itemize}
    \item $i \in \{1,2\}$: the microphone index
    \item $d_i$: time domain acoustic recording at microphone $i$
    \item $s$ the acoustic source
    \item $a_i$: the impulse response (IR) at microphone $i$  
    \item $u_i$: the noise at microphone $i$ 
\end{itemize}

Then we can define the RTF $H(k)$ as

\begin{equation}
    H(k) = \frac{A_1(k)}{A_2(k)}
\end{equation}

with $A_i(k)$ the Fourier transform of $a_i$ and $k$ the frequency.

Then $H(k)$ can be estimated by (with $d_1$ as reference):

\begin{equation}
        \hat{H}(k) = \frac{S_{d1d2}}{S_{d1d1}}
\end{equation}

with 

\begin{itemize}
    \item $S_{d1d1} = D_1(k)^{\ast} D_1(k)$: the Power Spectral Density (PSD)
    \item $S_{d2d1} = D_2(k)^{\ast} D_1(k)$: the cross-PSD for a single frame.
\end{itemize}

The estimator is biased since we ignore the PSD of the noise. For each FFT frame a vector is obtained of RTF is obtained $\hat{\mathbf{h}} = [\hat{H}(1), \dots, \hat{H}(K)]^T \in \mathbb{C}^K$ for $K$ frequencies bin. We use RTFs estimated using a single frame as input to the VAE-SSL.

the $n$th input sample and the supervised CNN is a sequence of RTFs frames :

\begin{equation}
        \mathbf{x}_n = \text{vec}(\text{phase}(\hat{\mathbf{H}}_n)) \in \mathbb{R}^{KP}
\end{equation}

with 

\begin{itemize}
    \item $\hat{\mathbf{H}}_n = [\hat{\mathbf{h}}_n \dots \hat{\mathbf{h}}_{n+P-1}] \in \mathbb{C}^{K \times P}$
    \item $K = N_{\text{FFT}}/2$
    \item $P$: the number of RTF frame in the sequence. 
\end{itemize}

\subsection{Neekhara}

\begin{itemize}
    \item \textbf{in:} (perceptually informed) spectogram (or text on a more basic level)
    \item \textbf{out:} natural sounding audio waveform
    \item \textbf{measurement scenario:} -
    \item \textbf{setup:} GAN (for amplitude estimation)
\end{itemize}

\cite{neekhara2019expediting} is concerned with finding a solution for Text to speech (TTS) problem. The claim is that using a GAN approach, they have been able to outperform by far naive approaches (user review) and being 100x faster than other DL approaches. More specifically this paper was concerned with creating a mapping from language to \textbf{perceptually informed spectogram}. Indeed the difficluty of the problem lays in the fact that perceptually informed spectograms are not invertible. Indeed a spectogram is a compact representation of a signal where much of the information contained in a audio waveform has been lost. More specifically the problem at hand is phase estimation and magnitude estimation. Therefore a predictive model is required to fill the missing information and create natural sounding sound.

\subsection{NEURIPS}

\begin{itemize}
    \item \textbf{in:} mel-spectogram 
    \item \textbf{out:} audio waveform
    \item \textbf{measurement scenario:} -
    \item \textbf{setup:} non-autoregressive feed-forward convolutional architecture to
    perform audio waveform generation in a GAN setup
\end{itemize}

\cite{NEURIPS2019_6804c9bc} also introduces a method for the TTS problem. Moreover in the introduction of the paper, there is a comparison of different method for text to speech (i.e. audio wave generation):

\begin{itemize}
    \item \textbf{Pure signal processing approaches:} "The main issue with these pure signal processing methods is that the mapping from intermediate features to audio usually \textbf{introduces noticeable artifacts}"
    \item \textbf{Autoregressive NN based models:} An autoregressive model is a model that relies on past values to predict current ones. In this sense, an autoregressive model must be sequential. "These methods have produced state-of-the-art results in text-to-speech synthesis and other audio generation tasks. Unfortunately, inference with these models is inherently slow and inefficient because audio samples must be generated sequentially. Thus auto-regressive models are \textbf{usually not suited for real-time applications}."

    \item \textbf{Non autoregressive models:} "While inference is fast on the GPU, the large size of the model makes it \textbf{ impractical for applications with a constrained memory budget.}"
    \item \textbf{GAN for audio:} However their results show that adversarial loss alone is not sufficient for high quality waveform generation; it requires a  KL-divergence based distillation objective as a critical component. To this date, making them work well in this domain has been challenging
\end{itemize}

In \cite{NEURIPS2019_6804c9bc}, a GAN (MelGAN) is introduced. The goal of this GAN is to perform audio waveform generation.

\subsection{EngelGanSynth:}

\begin{itemize}
    \item \textbf{in:} instruments note from datasets NSynth (Single channel )
    \item \textbf{out:} instruments audio waveform.
    \item \textbf{measurement scenario:} - 
    \item \textbf{setup:} GAN
\end{itemize}

\cite{engel2019gansynth} is concerned with demonstrating that GANs can in fact generate high-fidelity and locally-coherent naudio by modeling log magnitudes and instantaneous frequencies with sufficient frequency resolution in the spectral domain.

Using a GAN approach and the NSythn datasets (Dataset of standardized notes from instrument) to generate notes. 

TODO: According to Adam, this paper is to generate spectogram and not audio wave form. $\rightarrow$ check again

\subsection{gerstoft}

\begin{itemize}
    \item \textbf{in:} True Sample Covariance Matrix with DOA
    \item \textbf{out:} Generator, i.e. probability distribution function
    \item \textbf{measurement scenario:} -
    \item \textbf{setup:} (Wasserstein) GAN
\end{itemize}

\cite{gerstoft2020parametric} is concerned with training a GAN for generating audio based features. 

In many array processing techniques (i.e. beamfroming) first compute the Sample Covariance Matrix (SCM). The idea of the (Wasserstein) GAN here is to generate many SCM. More specifically, the goal of the GAN is to learn the \textbf{joint} probability distribution function of the \textbf{observable data} (array data) and the \textbf{target variable} (Wasserstein distance). The wasserstein distance is a metric to measure distance between probability distribution on a given metric space.

The idea is first to consider a model that describe relationship between \textbf{array data} and \textbf{DOA}. Using this model, another relationship betwen \textbf{CSM} and \textbf{array data} can be defined. Hence we have a mathematical relationship between \textbf{DOA} and \textbf{CSM}.

Idea: maybe it would be possible to generate for the pdf os SCM for several DOA. Moreover, a link between SCM and cross spectral matrix should be investigated.

\subsection{Vera-Diaz}

\begin{itemize}
    \item \textbf{in:} GCC
    \item \textbf{out:} (x,y,z)
    \item \textbf{measurement scenario:} pair of microphone
    \item \textbf{setup:} (Deep) CNN
\end{itemize}

In ASL prblem, the source’s position can be estimated with at least three Time Difference of Arrival (TDoA) measurements with hyperbolic trilateration methods. Signals captured in everyday scenarios are contaminated with noise and multipath effects.
Directly measuring TDoA in those cases is a diﬃcult task that produces inaccurate localization results.\cite{vera2021acoustic} is concerned with finding a way to denoise those signals. Other methods uses GCC instead of TDoA. SUch methods are more robust to noise and multipath effects, but not fully immune to it. 

The contribution of this paper is a DNN named DeepGCC. This DNN takes as input a GCC(Generalized Cross Correlation) Matrix and estimates a Gaussian function. Then the SSL problem is solved by replacing the GCC typically used by DeepGCC(GCC) and then using a classical beamforming approach. 

$\rightarrow$ this is actually the inverse of what we want. Hence if we could reverse this nectwork, maybe we could do something of the following: 

\begin{equation}
    (x,y,z) \rightarrow GCC \rightarrow CSM
\end{equation}

Where the last step is done thanks to the relation between $GCC$ and $CSM$ via Fourier transform. 

\subsection{Hübner}

\begin{itemize}
    \item \textbf{in:} -
    \item \textbf{out:} data for phase based DOA approximation. (RTF)
    \item \textbf{measurement scenario:} -
    \item \textbf{setup:} CNN
\end{itemize}

This paper is concerned with dealing with a common issue when creating DL algorithm to solve the SSL problem, namely the complexity to gather data for training/validating. Indeed, the two current way to obtain data is by either generating them using simulation or by recording them in real life. Both methods require significant amount of resources (resp. time or storage)

\textbf{TODO}: Use those quotes to structure a bit the the state-of-the-art

quotes from Hübner:
2
"DOA estimation methods can be categorized into classical model-based methods and data-driven methods, which are prevalently implemented using deep neural networks (DNN)."

"Most of the DL methods include a feature extraction step rather than using the raw microphone signals. Popular features include:

\begin{itemize}
    \item (i) the eigendecomposition of the spatial covariance matrix [14] (similar to MUSIC)
    \item (ii) generalized cross-correlation (GCC) based features [15–18]
    \item (iii) modal coherence [19]
    \item (iv) the Ambisonics intensity vectors [20]
    \item (v) phase and magnitude spectra [21] and
    \item (vi) phase spectra [11, 12]. Many of the features are phase-based as motivated by physical models and classical DOA estimators [9].
\end{itemize}
"

"One way to generate training data for DL-based DOA estimation is by recording sound emitted from a source (e.g., loudspeaker, human) in real acoustic environments [16, 17]. This approach is time-consuming and for high-quality datasets a precise ground truth position is essential, which requires expensive measurement equipment."

"Another popular method is the convolution of signals (e.g., speech) with room impulse responses (RIRs) that have either been recorded [14, 18] or simulated based on the source-image method [11, 12, 20, 21, 23]. The main drawbacks of these data generation methods are excessive time and storage consumption."


Conclusion: 
"We proposed a low complexity model-based training data
generation method for phase-based DOA estimation. The
proposed method models the microphone phases directly
in the frequency domain to avoid computationally costly
operations as present in state-of-the-art methods. The low
computational complexity of the proposed method allows for
online training data generation, which allows faster proto-
typing, and paves the way for \textbf{applications with a high data
demand} such as moving sound sources simulation or large
microphone arrays. An evaluation using measured RTFs
yielded \textbf{comparable results} for phase-based DOA estimation
when using \textbf{the proposed method and the computationally
expensive source-image method for training data generation.}"


\subsection{46107}

\cite{46107} is concerned with room simulation -> precisely not what we want to do. 

\subsection{Papayiannis}

\cite{papayiannis2019data} is concerned with the generation of Acoustic Impulse Response.

Reverberation are a good representation of the acoustic environment. In numerous taskm it is useful to be able to know in what environment a speech recording has been made (for instance), based on the reverbaration present in the recording. For this purpose, ML classifier have been built, in order to classify different enviroment. A feature that is typically used for such a classification is the Acoustic Impulse Response represented as Finite Impulse Response (FIR).

The issue is that a lot of IR are required for building a classifier. An IR is typically measured, hence the number that can be created is limited (time wise). For this reason, \cite{papayiannis2019data} introduce a way to generate (with GAN) artificial AIR from measured AIR. $\rightarrow$ data augmentation.

"This is an alternative to the process of measuring many more AIRs, by moving the source and receiver at various positions in the same real room. Repeating the process for a number of rooms expands the available dataset, without the need for any additional data collection"

"A challenge to overcome during training is related to the motivation for this work, which is the high-dimensionality of AIRs. This is overcome by using a proposed low-dimensional representation for acoustic environments. The representation describes sparse early reflection using the parameters estimated in [6] and uses established acoustic parameters to represent the late reverberation."

-> \textbf{Question for Adam: Are IR dependent on the source-receiver positions ? -> it is never mentionned in \cite{papayiannis2019data}}

\textbf{Data representation}: During training, AIRs are presented to the networks using \textbf{taps of FIR filters $\rightarrow$ to investigate}. The taps represent the sound pressure at the position of a receiver placed in the room, with the room excited by a source placed within its boundaries.

-> Hence the data generated is dependent on the dimension of the room. GAN trained for a specific room. But then the position of the receiver and source a randomly chosen (not conditional)

"The task of room classification is to identify a known room at unknown source and receiver positions. With the transformation being class invariant, the available training AIRs from a room will be used to artificially generate AIRs at new source and receiver positions from the same room."

$\rightarrow$ the main contribution of this paper is to propose a low dimension representation of AIR that is still informative (i.e. contains info about early reflection, late reverberation ) $\rightarrow$ look more into that (chapter: Proposed low-dimensional representation)

In this work, the generation of AIRs is based on training one GAN for each of the 7 rooms, part of the training database. Therefore, 7 GANs are trained and each one of them is used to generate a number of AIRs as if they were measured in the corresponding rooms.

\textbf{Conclusion:} as it is, this paper is not useful. For it to be useful for the task at hand, the GAN would need be conditionned on:

\begin{itemize}
    \item Source and Receiver positions 
    \item Room dimension (maybe less important)
\end{itemize}


\subsection{Ratnarajah}

\cite{ratnarajah2021fast} proposes a fast method (NN-based) for generating Room Impulse Response (RIR). The input paramaters used are the following:

\begin{itemize}
    \item rectangular room dimensions
    \item listener position
    \item speaker position
    \item reverberation time ($T_{60}$)
\end{itemize}

An issue with IR generator is that they are computationally very expensive. Moreover for NN training significant amount of data are required, hence offline data generation leads to high storage needs.

"To generate RIRs for a given acoustic environment, we propose a one-dimensional conditional generator network. Our generator network takes room geometry, listener and speaker positions, and $T_{60}$ as inputs, which are the common input used by all traditional RIR generators, and generates RIRs as raw-waveform audio. Our FAST-RIR generates RIRs of length 4096 at 16 kHz frequency."

$\rightarrow$ all together this paper seems to provide good results and seems applicable for the task at hand. $\rightarrow$ \textbf{TODO: write about this in State-of-the-Art alongside Papayiannis and ask Adam if it makes sense to keep both (probably not)} 


\subsection{Thoughts about the different data generation methods:}

In the thesis task, the following three data generation approach are listed (LHS) Whereas in the meeting, we mentionned the following three approaches (RHS):

\begin{itemize}
    \item Measurement $\leftrightarrow$ experimental measurements
    \item Syntetic $\leftrightarrow$ calculation of theoretical model [10]
    \item Semi-synthetic $\leftrightarrow$ virtual measurement [11]
\end{itemize}

In the state of the art, semi-synthetic approach must be mentionned explained and its limitation must be stated. Moreover a time were such an approach was used need to be mentionned.

Semi synthetic data generation approach are not the best suited for several reason. First, the data produce is not usually not as accurate as actual measurement. Moreover, generating sufficient data is extremely tedious and is often more time consuming than model training, since extremley significant quantity of data are required to train a NN. Indeed to have a good measurement a lot of IR must be created.

\textbf{Question: is semi synthetic when the RIR is measured and full synthetic when the RIR is generated with "maths" ? $\rightarrow$ wait for Adam email and use response to rewrite.}




\subsection{Difference between VAE and GAN}

In the context of image generation:

"GANs generally produce better photo-realistic images but can be difficult to work with. Conversely, VAEs are easier to train but don’t usually give the best results.

I recommend picking VAEs if you don’t have a lot of time to experiment with GANs and photorealism isn’t paramount.

There are exceptions such as Google’s VQ-VAE 2 which can compete with GANs for image quality and realism. There is also VAE-GAN and VQ-VAE-GAN.

As a note, GANs and VAEs are not specifically for images and can be used for other data types/structures." (this is from a comment on stackstats -> not really usable as a source).

TODO: $\rightarrow$ read \cite{10.1007/978-3-030-38961-1_8}

from \cite{papayiannis2019data}: "A generative model represents the joint probability $P(x, y)$, which is in contrast to classification DNNs that estimate the posterior $P(x|y)$. Recent advancements in deep learning led to the proposal of alternatives to the traditional method for the estimation of parametric model distributions. The two dominant methods in the modern literature are GANs and Variational Autoencoders (VAEs). Both follow a similar formulation that uses back-propagation to train network layers, which are able to estimate the generative model by filtering noise drawn from a known prior. In the literature review conducted for this work, GANs have shown to be widely adopted in the field of audio processing across different tasks such as SED [14], speech recognition [15], speech enhancement [16] and dereverberation [17, 18]. Furthermore, variants of the original GAN in [19] exist, which can be adapted in the future to lead to more exciting applications of the method proposed in this work, such as Conditional GANs [20], DualGANs [21] and many others. GANs are therefore chosen as the estimation mechanism for the generative models in this paper."


% =======================================================================================

\section{Goal}

The goal of the project is to create a GAN to generate either:
\begin{itemize}
    \item cross-spectral matrix
    \item the complex-valued sound pressure vector at the different microphone
\end{itemize}
with the corresponding labels (DoA).

In some way the goal of this project is to create a GAN to realistically add noise to either a cross spectral matrix or complex-valued signal sound pressure vector of an array of microphone. This noise pdf should be estimated instead of being modelled. Indeed a GAN approach is necessary to reduce the computation time and the storage required. Indeed we want to be able to generate randomly and in real time labeled data.  

\section{How to train a GAN:}

\begin{enumerate}
    \item feed sample to the discriminator to make it learn what a real sample is.
    \item When the discriminator can distinguish recognize real sample, we need to feed it fake sample and make sure it can recognize them as fake
    \item When the discriminator is good enough at his job, we can start training the generator. The generator takes as input a random input vector and create a sample
    \item The knowledge whether the sample was fake or not is revealed to both networks. Based upon this, the generator and discriminator need to adapt their behaviour:
    \begin{itemize}
        \item there is always a winner and a loser
        \item if the discriminator successfully detect the image as fake, it remains unchanged an5d the generator need to change its behaviour 
        \item On the other hand, if the discriminator fail to detect the image as fake, it has to adapt its behaviour and the generator stays unchanged. 
    \end{itemize}
    \item We repeat this process until the generator is so good that the discriminator can no longer detect the fakes.
\end{enumerate}

\section{Important words and relationships}

\begin{itemize}
    \item \textbf{SSL}: Sound source localization
    \item alternatively \textbf{SSL}: Semi-Supervised Learning
    \item \textbf{ASL}: Acoustics Source Localization    
    \item \textbf{Source Spectra}
    \item \textbf{GCC}: Generalized Cross Correlation
    \item \textbf{CSM}: Cross Spectral Matrix (note that there is a connection between the CSM and the GCC features via the Fourier transform)
    \item \textbf{SCM}: Sample Covariance Matrix
    \item \textbf{CB map}: Conventional Beamforming map (also known as Acoustic Power Map???)
    \item \textbf{conditioning variable}: splitting the data up into bins based on the values of these features, and then training a model for each bin. Then examining the differences between the models. Usually this is done to learn something about the benefits of using the different features, and about the relationships between features and outputs.
    \item \textbf{(Time) Stationary process}: process with mean, variance and autocorrelation structure not changing over time (constant overtime)
    \item \textbf{Autoregressive}: An autoregressive model specifies that the output variable depends linearly on its own previous values and on a stochastic term 
    \item Data Covariance Matrix $\leftrightarrow$ Cross Spectral Matrix $\leftrightarrow$ Cross Power Spectrum.
    \item relationship between pressure vector in the microphone array $\mathbf{p}$ and the Cross-Spectral Matrix $\mathbf{C}$
    \item SED: Sound Event Detection
    
    \begin{equation}
        \mathbf{C} = \mathbf{p} \mathbf{p}^H 
    \end{equation}
\end{itemize}

\section{Papers found summary:}

%=-%=-%=-%=-%=-%=-%=-%=-%=-%=-%=-%=-%=-%=-%=-%=-%=-%=-%=-%=-%=-%=-%=-%=-%=-%=-%=-%=-%=-%=
\subsection{Bianco}

In \cite{bianco2020semi}:

\begin{itemize} 
    \item \textbf{in:} Relative Transfer Function (RTF)
    \item \textbf{out:} Relative Transfer Function (RTF)
    \item \textbf{measurement scenario:} Binaural microphone.
    \item \textbf{setup:} VAE (Semi supervised learning)
\end{itemize}

\subsection{Neekhara}

In \cite{neekhara2019expediting}:

\begin{itemize}
    \item \textbf{in:} (perceptually informed) spectogram (or text on a more basic level)
    \item \textbf{out:} natural sounding audio waveform
    \item \textbf{measurement scenario:} -
    \item \textbf{setup:} GAN (for amplitude estimation)
\end{itemize}

\subsection{NEURIPS}

In \cite{NEURIPS2019_6804c9bc}:

\begin{itemize}
    \item \textbf{in:} mel-spectogram 
    \item \textbf{out:} audio waveform
    \item \textbf{measurement scenario:} -
    \item \textbf{setup:} non-autoregressive feed-forward convolutional architecture to
    perform audio waveform generation in a GAN setup
\end{itemize}

\subsection{Engel}

in \cite{engel2019gansynth}:

\begin{itemize}
    \item \textbf{in:} instruments note from datasets NSynth (Single channel)
    \item \textbf{out:} instruments audio waveform.
    \item \textbf{measurement scenario:} - 
    \item \textbf{setup:} GAN
\end{itemize}

\subsection{Gerstoft}

In \cite{gerstoft2020parametric}:

\begin{itemize}
    \item \textbf{in:} True Sample Covariance Matrix with DOA
    \item \textbf{out:} Generator, i.e. probability distribution function for Sample Covariance Matrix.
    \item \textbf{measurement scenario:} -
    \item \textbf{setup:} (Wasserstein) GAN
\end{itemize}

\subsection{Vera-Diaz}

In \cite{vera2021acoustic}:

\begin{itemize}
    \item \textbf{in:} GCC
    \item \textbf{out:} (x,y,z)
    \item \textbf{measurement scenario:} pair of microphone
    \item \textbf{setup:} (Deep) CNN
\end{itemize}

%=-%=-%=-%=-%=-%=-%=-%=-%=-%=-%=-%=-%=-%=-%=-%=-%=-%=-%=-%=-%=-%=-%=-%=-%=-%=-%=-%=-%=-%=




% Bibliography
%\addbibresource{mybib.bib}
\bibliography{../mybib}
\bibliographystyle{plain} 


\end{document}
